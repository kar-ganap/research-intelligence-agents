# Research Intelligence Platform: Complete Hackathon Plan
**Google Cloud Run Hackathon - Option B (Smart Stack)**  
*Last Updated: 2025-10-27*

---

## ğŸ“‹ Table of Contents

1. [Executive Summary](#executive-summary)
2. [Architecture Overview](#architecture-overview)
3. [Multi-Agent System Design](#multi-agent-system-design)
4. [Knowledge Graph Specification](#knowledge-graph-specification)
5. [Day-by-Day Implementation Plan](#day-by-day-implementation-plan)
6. [Knowledge Graph Visualization](#knowledge-graph-visualization)
7. [Demo Script](#demo-script)
8. [Risk Management](#risk-management)
9. [Submission Checklist](#submission-checklist)
10. [Quick Reference](#quick-reference)

---

## Executive Summary

### The Vision
A Research Intelligence Platform that builds a living map of research literature and proactively alerts researchers to important developments. Uses multi-agent collaboration built with Google ADK to ensure trustworthy, nuanced answers.

### Why This is A+

**The Core Value Proposition:**
> "What if a research assistant never slept? Our platform monitors arXiv 24/7, builds a knowledge graph of your field, and proactively alerts you when relevant papers publish. Researchers save 4.5 hours/week on literature review."

**Technical Excellence:**
- âœ… 7 specialized agents with sophisticated communication patterns
- âœ… All 3 Cloud Run resource types (Services, Jobs, Workers)
- âœ… Knowledge graph with relationship detection (novel approach)
- âœ… MapReduce pattern for graph updates
- âœ… Proactive intelligence (not just reactive Q&A)

**Differentiation:**
- **Knowledge Graph:** Most RAG systems don't build relationships between sources
- **Proactive Alerts:** System finds you, not vice versa
- **Contradiction Detection:** Shows uncertainty when papers disagree
- **Visual Excellence:** Interactive graph visualization

### Scope Decision: Option B + Phase 3 Pivot

**What we're building:**
- âœ… Knowledge Graph (medium depth)
- âœ… Proactive Alerting (simplified)
- âœ… Smart Q&A with confidence scoring
- âš ï¸ Debate system (only if ahead of schedule)

**What we're explicitly NOT building:**
- âŒ Full 3-team debate architecture (too complex, risky)
- âŒ GPU optimization (nice-to-have, not differentiating)
- âŒ MCP server (cool but not core to story)

---

## Architecture Overview

### The High-Level Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND (Cloud Run Service - Public)                       â”‚
â”‚ - React Dashboard with Knowledge Graph Visualization        â”‚
â”‚ - Real-time notifications (WebSockets optional)             â”‚
â”‚ - Watch rules management UI                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API GATEWAY (Cloud Run Service)                             â”‚
â”‚ - Authentication & rate limiting                            â”‚
â”‚ - Routes: /ask, /upload, /graph, /watch-rules              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORCHESTRATOR     â”‚          â”‚ GRAPH QUERY      â”‚
â”‚ (Cloud Run Svc)  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ (Cloud Run Svc)  â”‚
â”‚                  â”‚   A2A    â”‚                  â”‚
â”‚ Multi-Agent ADK: â”‚          â”‚ Specialized for  â”‚
â”‚ - RouterAgent    â”‚          â”‚ graph traversal  â”‚
â”‚ - RetrieverAgent â”‚          â”‚ and analysis     â”‚
â”‚ - AnswerAgent    â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ - ConfidenceAgentâ”‚
â”‚ - SynthesisAgent â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKGROUND JOBS (Cloud Run Jobs - Scheduled)                â”‚
â”‚                                                              â”‚
â”‚ 1. ARXIV WATCHER (daily 6am)                                â”‚
â”‚    Trigger: Cloud Scheduler                                 â”‚
â”‚    - Fetch new papers from arXiv RSS/API                    â”‚
â”‚    - Match against watch rules                              â”‚
â”‚    - Publish to Pub/Sub: arxiv.candidates                   â”‚
â”‚                                                              â”‚
â”‚ 2. INTAKE PIPELINE (triggered by arxiv.candidates)          â”‚
â”‚    Parallelization: --tasks=N (one per paper)               â”‚
â”‚    - Download & parse PDF                                   â”‚
â”‚    - Extract entities (methods, findings, datasets)         â”‚
â”‚    - Initial relationship detection                         â”‚
â”‚    - Store in Firestore + vector index                      â”‚
â”‚    - Publish to: docs.ready                                 â”‚
â”‚                                                              â”‚
â”‚ 3. KNOWLEDGE GRAPH UPDATE (daily 8pm)                       â”‚
â”‚    MapReduce Pattern:                                        â”‚
â”‚    - Map phase: Compute relationships for paper pairs       â”‚
â”‚    - Reduce phase: Aggregate, score confidence              â”‚
â”‚    - Detect paradigm shifts (citation velocity)             â”‚
â”‚    - Update Firestore relationships collection              â”‚
â”‚                                                              â”‚
â”‚ 4. WEEKLY DIGEST (Sunday 8am)                               â”‚
â”‚    - Aggregate week's papers per user                       â”‚
â”‚    - Generate personalized summaries                        â”‚
â”‚    - Send emails via SendGrid                               â”‚
â”‚                                                              â”‚
â”‚ 5. NIGHTLY EVAL (daily midnight)                            â”‚
â”‚    - Run test queries against labeled dataset               â”‚
â”‚    - Compute: extraction F1, citation coverage              â”‚
â”‚    - Store metrics in BigQuery                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ALERT WORKER (Cloud Run Worker - Pub/Sub consumer)          â”‚
â”‚ Pulls from: arxiv.matches                                   â”‚
â”‚ - Processes alert queue                                     â”‚
â”‚ - Sends emails (high priority)                              â”‚
â”‚ - WebSocket push (if user online)                           â”‚
â”‚ - Stores in alerts collection                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STORAGE LAYER                                                â”‚
â”‚                                                              â”‚
â”‚ Firestore Collections:                                      â”‚
â”‚ - papers/ (with extracted entities)                         â”‚
â”‚ - relationships/ (with confidence scores)                   â”‚
â”‚ - watch_rules/ (user-defined monitoring)                    â”‚
â”‚ - alerts/ (notification queue)                              â”‚
â”‚                                                              â”‚
â”‚ Cloud Storage Buckets:                                      â”‚
â”‚ - pdfs/ (original papers)                                   â”‚
â”‚ - pages/ (rendered page images)                             â”‚
â”‚ - overlays/ (citation visualizations)                       â”‚
â”‚ - fixtures/ (replay data for demos)                         â”‚
â”‚                                                              â”‚
â”‚ Vertex AI Vector Search:                                    â”‚
â”‚ - Embeddings for semantic search                            â”‚
â”‚ - Section-aware chunking                                    â”‚
â”‚                                                              â”‚
â”‚ BigQuery:                                                    â”‚
â”‚ - Time-series metrics                                       â”‚
â”‚ - Citation trends analysis                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cloud Run Resource Summary

**Services (4):**
1. `frontend` - React app
2. `api-gateway` - Auth + routing
3. `orchestrator` - Multi-agent Q&A
4. `graph-service` - Graph queries

**Jobs (5):**
1. `arxiv-watcher` - Daily monitoring
2. `intake-pipeline` - Parallel paper processing
3. `graph-updater` - MapReduce relationship detection
4. `weekly-digest` - Email generation
5. `nightly-eval` - Quality metrics

**Workers (1):**
1. `alert-worker` - Pub/Sub consumer for notifications

**Total: 10 Cloud Run deployments** (demonstrates mastery of all types)

---

## Multi-Agent System Design

### Agent Communication Patterns

**Pattern 1: Sequential Coordination**
```
RouterAgent â†’ RetrieverAgent â†’ AnswerAgent â†’ SynthesisAgent
```
- Fixed pipeline for deterministic flow
- Each agent's output becomes next agent's input
- Implemented via ADK's SequentialAgent

**Pattern 2: Parallel Consultation**
```
                    â”Œâ”€ RetrieverAgent â”€â”
AnswerAgent â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€ GraphQueryAgent â”œâ”€â”€â†’ SynthesisAgent
                    â””â”€ ContradictionAgent â”˜
```
- Three agents gather evidence simultaneously
- Results merged by SynthesisAgent
- Implemented via ADK's ParallelAgent

**Pattern 3: Feedback Loop**
```
AnswerAgent â†’ ConfidenceAgent â†’ [if < 0.6] â†’ RetrieverAgent â†’ AnswerAgent
```
- Conditional re-execution based on confidence
- Ensures quality before final answer
- Custom workflow logic in ADK

**Pattern 4: Event-Driven**
```
New Paper â†’ Intake Job â†’ Firestore â†’ Cloud Scheduler â†’ Re-answer saved questions
```
- Asynchronous agent triggering
- Cross-service communication via Pub/Sub
- Demonstrates distributed multi-agent system

### The 7 Core Agents

#### 1. RouterAgent (Entry Point)

**Role:** Classifies queries and routes to appropriate specialist

**Key Responsibilities:**
- Parse user question
- Extract key entities (methods, findings, topics)
- Classify question type:
  - `NUMERIC_QUERY`: "What was the accuracy?"
  - `COMPARISON_QUERY`: "Which is better, X or Y?"
  - `CONCEPTUAL_QUERY`: "How does diffusion work?"
  - `DISCOVERY_QUERY`: "Has anyone tried X?"
- Route to specialized handler

**ADK Implementation Pattern:**
```python
router_agent = LlmAgent(
    name="RouterAgent",
    model="gemini-2.0-flash-exp",
    description="Routes questions to specialized agents",
    instruction="""[See detailed prompt below]""",
    tools=[extract_entities_from_query],
    output_key="routing_decision"
)
```

**Success Metric:** >90% correct routing on test queries

---

#### 2. GraphQueryAgent (Knowledge Navigator)

**Role:** Explores knowledge graph for relevant context

**Key Responsibilities:**
- Find papers mentioning specific entities
- Traverse relationships (cites, supports, contradicts)
- Identify clusters of related work
- Return graph context to answer agent

**Tools Provided:**
```python
tools = [
    graph_lookup_papers,        # "Find all papers about X"
    graph_find_relationships,   # "What contradicts paper Y?"
    graph_traverse,             # "Papers within 2 hops of Z"
    graph_get_cluster           # "All papers in controversy cluster"
]
```

**Why This Matters:**
- Answers aren't just from retrieval, but from understanding paper relationships
- Can say: "Paper A supports this, but Paper B contradicts it"

---

#### 3. RetrieverAgent (Evidence Gatherer)

**Role:** Hybrid search across indexed papers

**Key Responsibilities:**
- Keyword search (BM25) for precise terms
- Vector search for semantic similarity
- Section-aware retrieval (prioritize Results/Discussion)
- Return top-K passages with source metadata

**Hybrid Strategy:**
```python
def hybrid_retrieve(query, k=5):
    # Keyword search
    bm25_results = keyword_search(query, k=10)
    
    # Vector search
    vector_results = vector_search(embed(query), k=10)
    
    # Merge with reciprocal rank fusion
    merged = reciprocal_rank_fusion([bm25_results, vector_results])
    
    return merged[:k]
```

**Success Metric:** Relevant passage in top-3 for 85% of queries

---

#### 4. ContradictionAgent (Trust Builder)

**Role:** Searches for conflicting evidence

**Key Responsibilities:**
- Query graph for "contradicts" relationships
- Look for papers with null results
- Find methodological criticisms
- Provide counter-evidence to proposed answer

**Why This Matters:**
- Prevents overconfident answers
- Builds trust through transparency
- Researchers need to know about disagreements

**Prompt Strategy:**
```
You are a scientific skeptic. Given a proposed answer,
search for papers that:
1. Report contradicting findings
2. Show null results for similar experiments
3. Criticize the methodology used

If contradictions found: Summarize them clearly
If not found: State "No contradictions in current corpus"
```

---

#### 5. ConfidenceAgent (Uncertainty Quantifier)

**Role:** Scores answer quality

**Scoring Formula:**
```python
def compute_confidence(context):
    base_score = 0.5
    
    # More supporting papers = higher confidence
    support_bonus = min(0.3, 0.05 * context['support_count'])
    
    # Contradictions = lower confidence
    contradiction_penalty = 0.2 * context['contradiction_count']
    
    # Data quality matters
    quality_bonus = 0.1 if context['has_table_data'] else 0
    
    # Recency matters
    recency_bonus = 0.1 if context['avg_year'] >= 2023 else 0
    
    confidence = base_score + support_bonus - contradiction_penalty 
                 + quality_bonus + recency_bonus
    
    return max(0.0, min(1.0, confidence))
```

**Thresholds:**
- `>0.8`: High confidence, answer directly
- `0.5-0.8`: Moderate confidence, include caveats
- `<0.5`: Low confidence, suggest need for more data

---

#### 6. SynthesisAgent (Answer Composer)

**Role:** Combines all inputs into final answer

**Input Structure:**
```python
{
    'routing_decision': {...},
    'retrieved_evidence': [...],
    'graph_context': {...},
    'contradictions': [...],
    'confidence_score': 0.75
}
```

**Output Format:**
```
[Direct Answer]
Drug X reduced HbA1c by 0.5% at 12 weeks [Paper A, Table 2].

[Supporting Evidence]
This finding is supported by two additional RCTs [Paper B, Paper C]
showing similar effect sizes.

[Nuance/Caveats]
Note: Paper D reported only 0.2% reduction, but used lower dosage.

[Confidence]
Confidence: 0.75 (Moderate-High)
Based on 3 supporting studies, 1 contradicting study.
```

**Critical Rule:** Every factual claim must have `[Source]` citation

---

#### 7. IngestorAgent (Background)

**Role:** Processes new papers (runs in Jobs, not interactive)

**Pipeline:**
```
PDF â†’ Text Extraction â†’ Entity Recognition â†’ Relationship Detection â†’ Indexing
```

**Sub-Agents:**
- FileLoaderAgent: Parse PDF to text
- TableExtractorAgent: Find and extract tables
- MetadataAgent: Extract authors, year, DOI
- EntityAgent: Extract methods, findings, datasets
- RelationshipAgent: Compare to existing papers
- IndexerAgent: Store in Firestore + vector DB

**Runs as:** Cloud Run Job with N parallel tasks

---

### Agent Orchestration Code (ADK)

```python
# Q&A Pipeline
qa_pipeline = SequentialAgent(
    name="QAPipeline",
    sub_agents=[
        # Step 1: Route
        router_agent,
        
        # Step 2: Gather evidence (parallel)
        ParallelAgent(
            name="EvidenceGathering",
            sub_agents=[
                retriever_agent,
                graph_query_agent,
                contradiction_agent
            ]
        ),
        
        # Step 3: Assess quality
        confidence_agent,
        
        # Step 4: Synthesize
        synthesis_agent
    ],
    description="Multi-agent Q&A system with trust mechanisms"
)

# Optional feedback loop
class AdaptiveQAPipeline(CustomAgent):
    def run(self, input):
        # First pass
        result = qa_pipeline.run(input)
        
        # Check confidence
        if result['confidence_score'] < 0.6:
            # Re-run with broader search
            enhanced_input = {
                **input,
                'search_mode': 'broad',
                'k': 10  # More results
            }
            result = qa_pipeline.run(enhanced_input)
        
        return result
```

---

## Knowledge Graph Specification

### Firestore Schema

#### Collection: `papers/`

```javascript
{
  // Primary identifiers
  paper_id: "arxiv:2310.12345",  // Stable ID
  arxiv_id: "2310.12345",
  doi: "10.1234/example",  // If available
  
  // Metadata
  title: "Diffusion Models for Sim-to-Real Transfer",
  authors: ["Smith, J.", "Lee, K.", "Chen, M."],
  published: "2023-10-15",  // ISO date
  year: 2023,
  
  // Content
  abstract: "...",
  full_text: "...",
  sections: {
    introduction: "...",
    methods: "...",
    results: "...",
    discussion: "...",
    conclusion: "..."
  },
  
  // Extracted entities
  entities: {
    methods: [
      "diffusion_models",
      "reinforcement_learning",
      "domain_randomization"
    ],
    findings: [
      "15%_improvement_rigid_objects",
      "works_poorly_on_deformables"
    ],
    datasets: [
      "robomimic",
      "metaworld",
      "d4rl"
    ],
    metrics: {
      "success_rate": 0.87,
      "sample_efficiency": "50k_steps"
    }
  },
  
  // For retrieval
  embeddings: {
    abstract: [...],  // 768-dim vector
    results_section: [...]
  },
  
  // Processing metadata
  pdf_url: "gs://bucket/pdfs/2310.12345.pdf",
  extracted_at: timestamp,
  extraction_version: "v1.2"
}
```

#### Collection: `relationships/`

```javascript
{
  // Identifiers
  relationship_id: "rel_20231015_001",
  source_paper: "arxiv:2310.12345",
  target_paper: "arxiv:2309.11111",
  
  // Relationship type
  relationship_type: "contradicts",
  // Options: "supports", "contradicts", "cites", "extends", "uses_method"
  
  // Evidence
  confidence: 0.85,  // 0-1 score
  evidence: `Source claims 15% improvement on rigid objects [Table 2].
             Target claims 3% improvement on same task [Figure 3].
             Both used RoboMimic dataset but different object types.`,
  
  // Specifics (conditional based on type)
  claim_details: {
    source_claim: "15% success rate improvement",
    target_claim: "3% success rate improvement",
    metric: "success_rate",
    conditions: {
      source: "rigid objects only",
      target: "including deformables"
    }
  },
  
  // Detection metadata
  detected_by: "llm_comparison",  // or: "citation_parsing", "manual"
  detected_at: timestamp,
  reviewed: false,  // For manual verification
  
  // For "uses_method" type
  method_name: "diffusion_models",
  
  // For temporal analysis
  citation_context: "criticizes", "builds_upon", "validates"
}
```

#### Collection: `watch_rules/`

```javascript
{
  // Identifiers
  rule_id: "user123_rule001",
  user_id: "user123",
  
  // Rule definition
  name: "Sim-to-Real Transfer Research",
  keywords: ["sim-to-real", "domain randomization", "transfer learning"],
  authors: ["Smith, J."],  // Optional
  exclude_keywords: ["theoretical", "survey"],  // Optional
  topics: ["cs.RO", "cs.LG"],  // arXiv categories
  
  // Matching criteria
  min_relevance_score: 0.7,  // 0-1 threshold
  
  // Notification settings
  frequency: "daily",  // "instant", "daily", "weekly"
  notification_method: "email",  // "email", "dashboard", "both"
  email: "researcher@university.edu",
  
  // State
  created_at: timestamp,
  last_triggered: timestamp,
  match_count: 15,
  active: true
}
```

#### Collection: `alerts/` (Queue)

```javascript
{
  // Identifiers
  alert_id: "alert_20231015_001",
  user_id: "user123",
  rule_id: "user123_rule001",
  paper_id: "arxiv:2310.12345",
  
  // Match information
  match_reason: "Keywords: sim-to-real (0.95), transfer learning (0.82)",
  relevance_score: 0.92,
  matched_keywords: ["sim-to-real", "transfer learning"],
  
  // Generated summary
  ai_summary: `New paper applies diffusion models to sim-to-real transfer,
                reporting 15% improvement over baselines on RoboMimic tasks.`,
  
  // Notification state
  created_at: timestamp,
  sent_at: timestamp,
  status: "pending",  // "pending", "sent", "read", "dismissed"
  delivery_method: "email"
}
```

### Relationship Detection Logic

**When to create relationships:**

1. **"cites"** - Automatic from bibliography parsing
2. **"supports"** - When findings align
   - Same method, similar results
   - Replication study
   - Meta-analysis inclusion
3. **"contradicts"** - When findings conflict
   - Same metric, different values (with significance)
   - Null results vs positive results
   - Different conclusions on same question
4. **"extends"** - When building upon
   - Improves method from prior paper
   - Adds new experiments to prior finding
   - Applies method to new domain
5. **"uses_method"** - When applying technique
   - Implements algorithm from prior paper
   - Uses dataset from prior paper

**Confidence Scoring:**

```python
def score_relationship_confidence(source_paper, target_paper, rel_type):
    """
    Returns confidence score 0-1 for proposed relationship
    """
    
    if rel_type == "cites":
        # Check if in bibliography
        return 1.0 if target_paper.id in source_paper.bibliography else 0.0
    
    elif rel_type in ["supports", "contradicts"]:
        # Use LLM to compare findings
        prompt = f"""
        Paper A: {source_paper.findings}
        Paper B: {target_paper.findings}
        
        Do these findings support or contradict each other?
        Score 0-1 for confidence.
        """
        
        llm_score = call_llm(prompt)
        
        # Boost if same dataset/method
        if overlap(source_paper.datasets, target_paper.datasets):
            llm_score = min(1.0, llm_score + 0.1)
        
        return llm_score
    
    elif rel_type == "extends":
        # Check for explicit "builds on" language
        extends_phrases = ["builds on", "extends", "improves upon"]
        if any(phrase in source_paper.introduction for phrase in extends_phrases):
            return 0.8
        return 0.3  # Low confidence default
    
    return 0.5  # Neutral default
```

### Graph Update Job (MapReduce)

**Map Phase: Pairwise Comparison**

```python
# Cloud Run Job: graph-updater
# Parameters: --tasks=N (one task per batch of paper pairs)

def map_task(paper_pairs, task_id):
    """
    Compare each pair and propose relationships
    """
    relationships = []
    
    for (paper_a, paper_b) in paper_pairs:
        # Skip if already compared
        if relationship_exists(paper_a.id, paper_b.id):
            continue
        
        # Check each relationship type
        for rel_type in ["supports", "contradicts", "extends"]:
            confidence = score_relationship_confidence(
                paper_a, paper_b, rel_type
            )
            
            if confidence > 0.6:  # Threshold
                relationships.append({
                    'source': paper_a.id,
                    'target': paper_b.id,
                    'type': rel_type,
                    'confidence': confidence,
                    'evidence': extract_evidence(paper_a, paper_b, rel_type)
                })
    
    # Write to temp collection
    firestore.collection('relationships_proposed').add_batch(relationships)
    
    return len(relationships)
```

**Reduce Phase: Aggregate & Validate**

```python
def reduce_task():
    """
    Aggregate proposed relationships and commit high-confidence ones
    """
    proposed = firestore.collection('relationships_proposed').stream()
    
    # Group by (source, target) pair
    grouped = defaultdict(list)
    for rel in proposed:
        key = (rel['source'], rel['target'])
        grouped[key].append(rel)
    
    committed = []
    for (source, target), rels in grouped.items():
        # If multiple detections, take highest confidence
        best = max(rels, key=lambda r: r['confidence'])
        
        if best['confidence'] > 0.7:  # Higher threshold for commit
            # Write to main relationships collection
            firestore.collection('relationships').add(best)
            committed.append(best)
    
    # Clean up temp collection
    firestore.collection('relationships_proposed').delete_all()
    
    # Detect paradigms
    detect_paradigm_shifts(committed)
    
    return len(committed)
```

**Paradigm Shift Detection:**

```python
def detect_paradigm_shifts(new_relationships):
    """
    Identify emerging trends from relationship patterns
    """
    # Count method mentions over time
    method_counts = defaultdict(lambda: defaultdict(int))
    
    papers = firestore.collection('papers').where('year', '>=', 2020).stream()
    for paper in papers:
        year = paper.to_dict()['year']
        for method in paper.to_dict().get('entities', {}).get('methods', []):
            method_counts[method][year] += 1
    
    # Detect acceleration
    paradigm_shifts = []
    for method, year_counts in method_counts.items():
        years = sorted(year_counts.keys())
        if len(years) < 3:
            continue
        
        # Calculate growth rate
        recent_count = sum(year_counts[y] for y in years[-2:])
        older_count = sum(year_counts[y] for y in years[:-2])
        
        if older_count > 0:
            growth_rate = recent_count / older_count
            
            if growth_rate > 3.0:  # 3x growth
                paradigm_shifts.append({
                    'method': method,
                    'growth_rate': growth_rate,
                    'recent_papers': recent_count,
                    'status': 'emerging_paradigm'
                })
    
    # Store insights
    if paradigm_shifts:
        firestore.collection('insights').document('paradigm_shifts').set({
            'detected_at': datetime.now(),
            'shifts': paradigm_shifts
        })
    
    return paradigm_shifts
```

---

## Day-by-Day Implementation Plan

### Pre-Hackathon (Evening Before)

**Critical Setup (2-3 hours):**

- [ ] ADK Installation
  ```bash
  pip install google-adk
  adk --version
  ```

- [ ] Test basic agent
  ```python
  from google.adk import LlmAgent
  
  test_agent = LlmAgent(
      name="TestAgent",
      model="gemini-2.0-flash-exp",
      instruction="Say hello"
  )
  
  result = test_agent.run("Test")
  print(result)  # Should work
  ```

- [ ] Firestore setup
  ```bash
  # Create collections (do this in Firebase Console)
  # papers/, relationships/, watch_rules/, alerts/
  ```

- [ ] PDF parsing test
  ```python
  import fitz  # PyMuPDF
  
  # Test on 5 real arXiv papers
  # Identify edge cases:
  # - Scanned PDFs (images, not text)
  # - Multi-column layouts
  # - Tables as images vs text tables
  ```

- [ ] Cloud Run "hello world"
  ```bash
  # Deploy basic service
  gcloud run deploy test-service \
    --image gcr.io/cloudrun/hello \
    --region us-central1
  ```

**Success Criteria:**
- ADK runs locally without errors
- Can write to Firestore
- Can extract text from at least 3 PDFs
- Cloud Run deployment works

---

### Day 1: Foundation

**Goal:** Papers in, structured data out, basic Q&A works

#### Morning (9am - 1pm): Basic Ingestion

**Hours 1-2: File Reading**

- [ ] Create `tools/pdf_reader.py`
  ```python
  def read_pdf(file_path: str) -> dict:
      """
      Extract text from PDF
      Returns: {
          'text': full_text,
          'pages': [page1_text, page2_text, ...],
          'page_count': N
      }
      """
      import fitz
      
      doc = fitz.open(file_path)
      pages = []
      
      for page_num, page in enumerate(doc):
          pages.append({
              'page_number': page_num + 1,
              'text': page.get_text()
          })
      
      full_text = '\n\n'.join(p['text'] for p in pages)
      
      return {
          'text': full_text,
          'pages': pages,
          'page_count': len(pages)
      }
  ```

- [ ] Test on 3 papers, handle errors gracefully

**Hours 3-4: Entity Extraction**

- [ ] Create `agents/extraction_agent.py`
  ```python
  extraction_agent = LlmAgent(
      name="EntityExtractor",
      model="gemini-2.0-flash-exp",
      instruction="""
      Extract structured information from this research paper.
      
      Return JSON with:
      {
          "methods": ["method1", "method2"],
          "findings": ["finding1", "finding2"],
          "datasets": ["dataset1"],
          "metrics": {
              "accuracy": 0.95,
              "f1_score": 0.87
          }
      }
      
      Be specific. Use lowercase_snake_case for method names.
      """
  )
  ```

- [ ] Run on 3 papers
- [ ] Store results in Firestore
- [ ] Verify data appears in Firebase Console

**Lunch (1pm - 2pm)**

#### Afternoon (2pm - 6pm): Knowledge Graph Foundation

**Hours 1-2: Relationship Detection (Basic)**

- [ ] Create `agents/relationship_agent.py`
  ```python
  def detect_relationship(paper_a: dict, paper_b: dict) -> dict:
      """
      Compare two papers and detect relationships
      """
      
      prompt = f"""
      Compare these two papers:
      
      Paper A:
      Title: {paper_a['title']}
      Findings: {paper_a['entities']['findings']}
      Methods: {paper_a['entities']['methods']}
      
      Paper B:
      Title: {paper_b['title']}
      Findings: {paper_b['entities']['findings']}
      Methods: {paper_b['entities']['methods']}
      
      Do they:
      1. Support each other (similar findings)
      2. Contradict each other (conflicting findings)
      3. Neither (unrelated or unclear)
      
      Return JSON:
      {{
          "relationship": "supports|contradicts|none",
          "confidence": 0.0-1.0,
          "evidence": "brief explanation"
      }}
      """
      
      result = call_llm(prompt)
      return result
  ```

- [ ] Test on 2 paper pairs (one supporting, one contradicting)
- [ ] Store in `relationships/` collection

**Hours 3-4: Cloud Run Jobs Setup**

- [ ] Create `jobs/intake.py`
  ```python
  # Simple job that processes one paper
  import sys
  
  def main():
      paper_url = sys.argv[1]
      
      # Download PDF
      pdf_path = download_pdf(paper_url)
      
      # Extract text
      text_data = read_pdf(pdf_path)
      
      # Extract entities
      entities = extraction_agent.run(text_data['text'])
      
      # Store in Firestore
      paper_doc = {
          'paper_id': generate_id(paper_url),
          'url': paper_url,
          'extracted_at': datetime.now(),
          **entities
      }
      
      firestore.collection('papers').add(paper_doc)
      
      print(f"Processed: {paper_doc['paper_id']}")
  
  if __name__ == "__main__":
      main()
  ```

- [ ] Build Docker image
  ```dockerfile
  FROM python:3.11-slim
  
  WORKDIR /app
  
  COPY requirements.txt .
  RUN pip install -r requirements.txt
  
  COPY . .
  
  CMD ["python", "jobs/intake.py"]
  ```

- [ ] Deploy to Cloud Run Jobs
  ```bash
  gcloud run jobs create intake-job \
    --image gcr.io/PROJECT/intake \
    --region us-central1 \
    --task-timeout 15m \
    --max-retries 1
  ```

- [ ] Test execution
  ```bash
  gcloud run jobs execute intake-job \
    --args="https://arxiv.org/pdf/2310.12345.pdf"
  ```

#### Evening (6pm - 9pm): Basic Q&A

**Hours 1-2: Simple Retriever**

- [ ] Create `tools/retrieval.py`
  ```python
  def keyword_search(query: str, limit: int = 5) -> list:
      """
      Simple keyword search in Firestore
      """
      papers = firestore.collection('papers').stream()
      
      results = []
      query_terms = query.lower().split()
      
      for paper in papers:
          data = paper.to_dict()
          text = (data.get('title', '') + ' ' + 
                  ' '.join(data.get('entities', {}).get('findings', [])))
          text = text.lower()
          
          # Count keyword matches
          matches = sum(term in text for term in query_terms)
          
          if matches > 0:
              results.append({
                  'paper_id': paper.id,
                  'title': data['title'],
                  'findings': data.get('entities', {}).get('findings', []),
                  'score': matches
              })
      
      # Sort by score
      results.sort(key=lambda x: x['score'], reverse=True)
      return results[:limit]
  ```

**Hours 2-3: Answer Agent**

- [ ] Create `agents/answer_agent.py`
  ```python
  answer_agent = LlmAgent(
      name="AnswerAgent",
      model="gemini-2.0-flash-exp",
      instruction="""
      Answer the question based on the retrieved evidence.
      
      Rules:
      1. Only use information from the evidence provided
      2. Cite sources as [Paper Title]
      3. If evidence insufficient, say "I don't have enough information"
      4. Be concise (2-3 sentences)
      
      Evidence: {evidence}
      Question: {question}
      
      Answer:
      """,
      tools=[],
      output_key="answer"
  )
  ```

- [ ] Create end-to-end pipeline
  ```python
  def answer_question(question: str) -> dict:
      # Retrieve
      evidence = keyword_search(question)
      
      # Format evidence
      evidence_text = '\n'.join([
          f"- {r['title']}: {', '.join(r['findings'])}"
          for r in evidence
      ])
      
      # Answer
      result = answer_agent.run({
          'evidence': evidence_text,
          'question': question
      })
      
      return {
          'question': question,
          'answer': result['answer'],
          'sources': [r['title'] for r in evidence]
      }
  ```

- [ ] Test on 3 questions
  - "What methods are used for sim-to-real transfer?"
  - "What accuracy was achieved?"
  - "Which datasets were used?"

**Day 1 Success Criteria:**
- [ ] 3 papers in Firestore with extracted entities
- [ ] At least 1 relationship detected and stored
- [ ] Can ask question and get answer with citation
- [ ] Cloud Run Job successfully processes 1 paper

**Commit & Push:** End of Day 1 working state

---

### Day 2: Intelligence Layer

**Goal:** Proactive alerts work, graph queries enhance answers, confidence scoring

#### Morning (9am - 1pm): Watch Rules + Alerting

**Hour 1: Watch Rules CRUD**

- [ ] Create `api/watch_rules.py`
  ```python
  @app.route('/api/watch-rules', methods=['GET', 'POST'])
  def watch_rules():
      user_id = get_current_user_id()
      
      if request.method == 'GET':
          rules = firestore.collection('watch_rules')\
              .where('user_id', '==', user_id)\
              .stream()
          return jsonify([r.to_dict() for r in rules])
      
      elif request.method == 'POST':
          data = request.json
          
          rule = {
              'rule_id': f"{user_id}_rule{timestamp}",
              'user_id': user_id,
              'name': data['name'],
              'keywords': data['keywords'],
              'frequency': data.get('frequency', 'daily'),
              'created_at': datetime.now(),
              'active': True
          }
          
          firestore.collection('watch_rules').add(rule)
          return jsonify(rule), 201
  ```

- [ ] Simple UI for creating rules (can be basic form)

**Hours 2-3: Matching Logic**

- [ ] Create `jobs/arxiv_watcher.py`
  ```python
  def check_new_papers():
      """
      Fetch new papers from arXiv and match against watch rules
      """
      # Fetch papers from last 24 hours
      yesterday = datetime.now() - timedelta(days=1)
      
      # For demo: use fixture
      papers = load_arxiv_fixture()  # Or fetch_arxiv_papers()
      
      # Get all active watch rules
      rules = firestore.collection('watch_rules')\
          .where('active', '==', True)\
          .stream()
      
      alerts_created = 0
      
      for rule in rules:
          rule_data = rule.to_dict()
          
          for paper in papers:
              # Simple keyword matching
              score = calculate_match_score(paper, rule_data['keywords'])
              
              if score > 0.7:  # Threshold
                  alert = {
                      'alert_id': generate_id(),
                      'user_id': rule_data['user_id'],
                      'rule_id': rule_data['rule_id'],
                      'paper_id': paper['id'],
                      'match_reason': f"Keywords: {', '.join(rule_data['keywords'])}",
                      'relevance_score': score,
                      'created_at': datetime.now(),
                      'status': 'pending'
                  }
                  
                  firestore.collection('alerts').add(alert)
                  alerts_created += 1
      
      return alerts_created
  ```

- [ ] Deploy as Cloud Run Job (daily schedule)
  ```bash
  gcloud scheduler jobs create http arxiv-watch-daily \
    --schedule="0 6 * * *" \
    --uri="https://jobs-service.run.app/run-arxiv-watch" \
    --http-method=POST
  ```

**Hour 4: Alert Worker**

- [ ] Create `workers/alert_worker.py`
  ```python
  from google.cloud import pubsub_v1
  
  def process_alert(alert_data):
      """
      Send notification for an alert
      """
      user_id = alert_data['user_id']
      paper_id = alert_data['paper_id']
      
      # Get user info
      user = firestore.collection('users').document(user_id).get()
      email = user.to_dict()['email']
      
      # Get paper info
      paper = firestore.collection('papers').document(paper_id).get()
      paper_data = paper.to_dict()
      
      # Send email
      send_email(
          to=email,
          subject=f"New paper matches your watch rule",
          body=f"""
          A new paper matches your interests:
          
          Title: {paper_data['title']}
          Authors: {', '.join(paper_data['authors'])}
          
          Relevance: {alert_data['relevance_score']:.0%}
          Reason: {alert_data['match_reason']}
          
          [View Paper](link)
          """
      )
      
      # Update alert status
      firestore.collection('alerts')\
          .document(alert_data['alert_id'])\
          .update({'status': 'sent', 'sent_at': datetime.now()})
  
  # Pub/Sub subscriber
  def callback(message):
      alert_data = json.loads(message.data)
      process_alert(alert_data)
      message.ack()
  
  subscriber = pubsub_v1.SubscriberClient()
  subscription_path = subscriber.subscription_path(PROJECT_ID, 'alert-subscription')
  
  future = subscriber.subscribe(subscription_path, callback=callback)
  print(f"Listening for alerts on {subscription_path}")
  
  try:
      future.result()
  except KeyboardInterrupt:
      future.cancel()
  ```

- [ ] Deploy as Cloud Run Worker
  ```bash
  gcloud run deploy alert-worker \
    --image gcr.io/PROJECT/alert-worker \
    --region us-central1 \
    --min-instances=1 \
    --max-instances=10
  ```

**Lunch (1pm - 2pm)**

#### Afternoon (2pm - 6pm): Smart Agents

**Hours 1-2: GraphQueryAgent**

- [ ] Create `agents/graph_agent.py`
  ```python
  def graph_lookup_papers(entity: str) -> list:
      """
      Find papers mentioning an entity
      """
      papers = firestore.collection('papers')\
          .where('entities.methods', 'array_contains', entity)\
          .stream()
      
      return [p.to_dict() for p in papers]
  
  def graph_find_relationships(paper_id: str, rel_type: str = None) -> list:
      """
      Find relationships for a paper
      """
      query = firestore.collection('relationships')\
          .where('source_paper', '==', paper_id)
      
      if rel_type:
          query = query.where('relationship_type', '==', rel_type)
      
      return [r.to_dict() for r in query.stream()]
  
  graph_agent = LlmAgent(
      name="GraphQueryAgent",
      model="gemini-2.0-flash-exp",
      instruction="""
      You have access to a knowledge graph.
      Use the tools to explore relationships between papers.
      
      Available tools:
      - graph_lookup_papers(entity): Find papers about an entity
      - graph_find_relationships(paper_id, type): Find how papers relate
      """,
      tools=[graph_lookup_papers, graph_find_relationships]
  )
  ```

**Hours 2-3: ContradictionAgent**

- [ ] Create `agents/contradiction_agent.py`
  ```python
  def find_contradictions(paper_id: str, claim: str) -> list:
      """
      Search for papers that contradict a claim
      """
      # Get all contradiction relationships
      contradictions = firestore.collection('relationships')\
          .where('source_paper', '==', paper_id)\
          .where('relationship_type', '==', 'contradicts')\
          .stream()
      
      results = []
      for rel in contradictions:
          rel_data = rel.to_dict()
          
          # Get the contradicting paper
          target_paper = firestore.collection('papers')\
              .document(rel_data['target_paper'])\
              .get()
          
          results.append({
              'paper': target_paper.to_dict(),
              'evidence': rel_data['evidence'],
              'confidence': rel_data['confidence']
          })
      
      return results
  
  contradiction_agent = LlmAgent(
      name="ContradictionAgent",
      model="gemini-2.0-flash-exp",
      instruction="""
      Search for papers that contradict the proposed answer.
      
      If contradictions found:
      - Summarize the conflicting finding
      - Explain why it differs (if known)
      
      If none found:
      - State "No contradictions found in current corpus"
      """,
      tools=[find_contradictions]
  )
  ```

**Hour 4: ConfidenceAgent**

- [ ] Create `agents/confidence_agent.py`
  ```python
  def compute_confidence(context: dict) -> dict:
      """
      Score answer confidence based on evidence
      """
      score = 0.5  # Base
      
      # Supporting papers
      support_count = len(context.get('supporting_papers', []))
      score += min(0.3, 0.05 * support_count)
      
      # Contradictions
      contradiction_count = len(context.get('contradictions', []))
      score -= 0.2 * contradiction_count
      
      # Data quality
      if context.get('has_table_data'):
          score += 0.1
      
      # Recency
      if context.get('avg_year', 2020) >= 2023:
          score += 0.1
      
      # Clamp to [0, 1]
      score = max(0.0, min(1.0, score))
      
      # Determine recommendation
      if score > 0.8:
          recommendation = "answer_with_high_confidence"
      elif score > 0.5:
          recommendation = "answer_with_caveats"
      else:
          recommendation = "need_more_data"
      
      return {
          'confidence_score': score,
          'recommendation': recommendation,
          'reasoning': f"Based on {support_count} supporting papers, "
                      f"{contradiction_count} contradictions"
      }
  
  confidence_agent = LlmAgent(
      name="ConfidenceAgent",
      model="gemini-2.0-flash-exp",
      instruction="""
      Score the proposed answer's confidence.
      
      Consider:
      - Number of supporting papers (more = better)
      - Contradictions (any = lower confidence)
      - Data quality (table > narrative)
      - Recency (newer = better)
      
      Return confidence score 0-1 and recommendation.
      """,
      tools=[compute_confidence]
  )
  ```

#### Evening (6pm - 9pm): Graph Update Job

**Hours 1-2: MapReduce Setup**

- [ ] Create `jobs/graph_updater.py`
  ```python
  def map_compare_papers(paper_batch, task_id):
      """
      Map task: Compare papers in batch
      """
      relationships = []
      
      # Get all papers for comparison
      all_papers = list(firestore.collection('papers').stream())
      paper_docs = {p.id: p.to_dict() for p in all_papers}
      
      # Process assigned batch
      for paper_a_id in paper_batch:
          paper_a = paper_docs[paper_a_id]
          
          for paper_b_id, paper_b in paper_docs.items():
              if paper_a_id >= paper_b_id:  # Avoid duplicates
                  continue
              
              # Check for relationships
              rel = detect_relationship(paper_a, paper_b)
              
              if rel['relationship'] != 'none' and rel['confidence'] > 0.6:
                  relationships.append({
                      'source_paper': paper_a_id,
                      'target_paper': paper_b_id,
                      'relationship_type': rel['relationship'],
                      'confidence': rel['confidence'],
                      'evidence': rel['evidence'],
                      'detected_at': datetime.now()
                  })
      
      # Write to temp collection
      for rel in relationships:
          firestore.collection('relationships_proposed').add(rel)
      
      return len(relationships)
  
  if __name__ == "__main__":
      task_id = int(os.environ.get('CLOUD_RUN_TASK_INDEX', 0))
      task_count = int(os.environ.get('CLOUD_RUN_TASK_COUNT', 1))
      
      # Get all paper IDs
      papers = [p.id for p in firestore.collection('papers').stream()]
      
      # Shard by task
      batch_size = len(papers) // task_count
      start = task_id * batch_size
      end = start + batch_size if task_id < task_count - 1 else len(papers)
      
      my_batch = papers[start:end]
      
      count = map_compare_papers(my_batch, task_id)
      print(f"Task {task_id} created {count} proposed relationships")
  ```

**Hours 2-3: Reduce + Paradigm Detection**

- [ ] Add reduce logic
  ```python
  def reduce_aggregate_relationships():
      """
      Reduce task: Aggregate and commit relationships
      """
      proposed = firestore.collection('relationships_proposed').stream()
      
      # Group duplicates
      seen = {}
      for rel in proposed:
          rel_data = rel.to_dict()
          key = (rel_data['source_paper'], rel_data['target_paper'])
          
          if key not in seen or rel_data['confidence'] > seen[key]['confidence']:
              seen[key] = rel_data
      
      # Commit high-confidence relationships
      committed = 0
      for rel_data in seen.values():
          if rel_data['confidence'] > 0.7:
              firestore.collection('relationships').add(rel_data)
              committed += 1
      
      # Clean up temp collection
      batch = firestore.batch()
      for doc in firestore.collection('relationships_proposed').stream():
          batch.delete(doc.reference)
      batch.commit()
      
      print(f"Committed {committed} relationships")
      
      # Detect paradigm shifts
      detect_paradigm_shifts()
      
      return committed
  ```

**Day 2 Success Criteria:**
- [ ] Watch rule created via API
- [ ] Alert triggered and email sent (test with real email)
- [ ] ContradictionAgent finds contradiction (if exists)
- [ ] ConfidenceAgent returns score
- [ ] Graph update job runs without errors

**Commit & Push:** End of Day 2 working state

---

### Day 3: Polish + Phase 3 Decision

**Goal:** Decide on additional features vs polish, create demo-ready state

#### Morning (9am - 12pm): THE PIVOT DECISION

**9am - 9:30am: Status Assessment**

Run through checklist:

- [ ] Can ingest paper end-to-end? (Test on new paper)
- [ ] Can answer question with citations? (Test 3 questions)
- [ ] Can trigger alert? (Test one rule)
- [ ] Graph shows relationships? (Check Firestore)
- [ ] All services deployed? (Check Cloud Run console)
- [ ] Any critical bugs? (List them)

**Calculate buffer:**
```
Remaining time = 48 hours total - hours spent - 8 hours (Day 4 buffer)
Buffer = Remaining time - (critical bugs Ã— 1 hour each)
```

**Decision Tree:**

```
IF buffer > 6 hours:
    â†’ Add knowledge graph visualization (3-4 hours)
    â†’ Polish UI (2 hours)

ELSE IF buffer > 3 hours:
    â†’ Add graph visualization OR polish extensively
    â†’ Choose based on: which looks better in demo?

ELSE:
    â†’ Skip all extras
    â†’ Fix bugs
    â†’ Polish demo flow
    â†’ Rehearse
```

**9:30am - 12pm: Execute Decision**

**Option A: Add Graph Visualization** (if ahead)

- [ ] Install vis.js
  ```bash
  npm install vis-network
  ```

- [ ] Create API endpoint
  ```python
  @app.route('/api/graph')
  def get_graph_data():
      papers = firestore.collection('papers').stream()
      relationships = firestore.collection('relationships').stream()
      
      # Format for vis.js (see detailed spec below)
      nodes = [...]
      edges = [...]
      
      return jsonify({'nodes': nodes, 'edges': edges})
  ```

- [ ] Create React component (see full implementation in section below)

- [ ] Test:
  - [ ] Graph loads
  - [ ] Can click nodes
  - [ ] Filters work
  - [ ] Looks good in demo

**Option B: Extensive Polish** (if on track)

- [ ] UI consistency pass
  - [ ] All buttons same style
  - [ ] Consistent spacing
  - [ ] Loading states everywhere
  - [ ] Error messages friendly

- [ ] Demo data curation
  - [ ] 10 papers pre-loaded
  - [ ] 5 watch rules examples
  - [ ] 3 pre-crafted questions

- [ ] Demo rehearsal
  - [ ] Run through 3 times
  - [ ] Time each part
  - [ ] Fix any hiccups

**Lunch (12pm - 1pm)**

#### Afternoon (1pm - 6pm): Demo Preparation

**Hours 1-2: Weekly Digest**

- [ ] Create `jobs/weekly_digest.py`
  ```python
  def generate_weekly_digest(user_id):
      """
      Create personalized weekly summary
      """
      # Get papers from last week
      week_ago = datetime.now() - timedelta(days=7)
      
      papers = firestore.collection('papers')\
          .where('extracted_at', '>=', week_ago)\
          .stream()
      
      # Get user's watch rules
      rules = firestore.collection('watch_rules')\
          .where('user_id', '==', user_id)\
          .stream()
      
      # Match papers to rules
      relevant_papers = []
      for paper in papers:
          paper_data = paper.to_dict()
          for rule in rules:
              rule_data = rule.to_dict()
              if matches_rule(paper_data, rule_data):
                  relevant_papers.append(paper_data)
                  break
      
      # Generate summary
      if not relevant_papers:
          return None
      
      summary = f"""
      # Your Weekly Research Digest
      
      {len(relevant_papers)} papers matched your interests this week.
      
      ## Highlights:
      """
      
      for paper in relevant_papers[:5]:
          summary += f"""
          ### {paper['title']}
          {paper['authors'][0]} et al.
          
          Key findings: {', '.join(paper['entities']['findings'][:2])}
          
          ---
          """
      
      return summary
  ```

**Hours 3-4: Dashboard UI**

- [ ] Create main dashboard page
  ```jsx
  function Dashboard() {
      return (
          <div className="container mx-auto p-6">
              <h1 className="text-3xl font-bold mb-6">
                  Research Intelligence Dashboard
              </h1>
              
              <div className="grid grid-cols-3 gap-6">
                  {/* Stats cards */}
                  <StatCard 
                      title="Papers Indexed" 
                      value="47" 
                      change="+5 this week"
                  />
                  <StatCard 
                      title="Active Watch Rules" 
                      value="3"
                  />
                  <StatCard 
                      title="Alerts This Week" 
                      value="12"
                  />
              </div>
              
              {/* Knowledge graph */}
              <div className="mt-8">
                  <h2 className="text-2xl font-semibold mb-4">
                      Knowledge Graph
                  </h2>
                  <KnowledgeGraph />
              </div>
              
              {/* Recent alerts */}
              <div className="mt-8">
                  <h2 className="text-2xl font-semibold mb-4">
                      Recent Alerts
                  </h2>
                  <AlertList />
              </div>
          </div>
      );
  }
  ```

**Hours 5-6: Demo Script Rehearsal**

- [ ] Write demo script (see section below)
- [ ] Practice 3x
- [ ] Time each section
- [ ] Identify rough spots
- [ ] Fix issues

#### Evening (6pm - 10pm): Video + Content

**Hours 1-2: Record Demo Video**

- [ ] Set up recording environment
  - Clean browser
  - Close unnecessary tabs
  - Full screen
  - High resolution

- [ ] Record in segments (easier to edit)
  - Segment 1: Overview (30s)
  - Segment 2: arXiv watch (45s)
  - Segment 3: Q&A with citations (45s)
  - Segment 4: Knowledge graph (60s)

- [ ] Record multiple takes of each

**Hours 2-3: Blog Post Draft**

- [ ] Title: "Building a Research Intelligence Platform with Multi-Agent Systems on Google Cloud Run"

- [ ] Structure:
  1. Introduction (the problem)
  2. Architecture overview
  3. Multi-agent design
  4. Knowledge graph approach
  5. Demo highlights
  6. Results & learnings
  7. Next steps

- [ ] Include:
  - Architecture diagram
  - Code snippets of agent definitions
  - Screenshots of dashboard
  - Cost analysis

**Hour 3: Architecture Diagram**

- [ ] Create in Excalidraw or Lucidchart
- [ ] Show all Cloud Run resources
- [ ] Show data flow
- [ ] Show agent communication
- [ ] Export as PNG

**Hour 4: GitHub README**

```markdown
# Research Intelligence Platform

AI-powered system that monitors research literature, builds knowledge graphs, and provides proactive intelligence to researchers.

## Architecture

[Insert diagram]

## Features

- ğŸ“š Automatic paper ingestion from arXiv
- ğŸ•¸ï¸ Knowledge graph with relationship detection
- ğŸ”” Proactive alerts for relevant papers
- ğŸ’¬ Multi-agent Q&A with citations
- ğŸ“Š Interactive graph visualization

## Tech Stack

- Google Cloud Run (Services, Jobs, Workers)
- Google ADK (Multi-agent orchestration)
- Firestore (Knowledge graph storage)
- Gemini 2.0 (LLM reasoning)
- React + vis.js (Frontend)

## Quick Start

[Setup instructions]

## Demo Video

[Link to video]

## Blog Post

[Link to Medium post]
```

**Day 3 Success Criteria:**
- [ ] Demo runs flawlessly 3x in a row
- [ ] Video recorded and edited
- [ ] Blog post drafted
- [ ] Architecture diagram created
- [ ] GitHub README written

**Commit & Push:** End of Day 3 working state

---

### Day 4: Submission + Buffer

#### Morning (9am - 12pm): Final Polish

- [ ] **Bug fixes**
  - Any issues from last night?
  - Test on fresh browser
  - Test on demo laptop

- [ ] **Deploy to production**
  ```bash
  # Final deployment
  gcloud run deploy frontend --region us-central1
  gcloud run deploy api-gateway --region us-central1
  gcloud run deploy orchestrator --region us-central1
  ```

- [ ] **Test public URLs**
  - Frontend loads
  - Can create account
  - Can ask question
  - Can view graph

- [ ] **Final video edit**
  - Add intro/outro
  - Add captions
  - Export in high quality
  - Upload to YouTube

#### Afternoon (12pm - 3pm): Submission

- [ ] **Prepare submission materials**
  - âœ… Text description
  - âœ… Demo video URL
  - âœ… GitHub repository (public)
  - âœ… Architecture diagram
  - âœ… "Try it out" link
  - âœ… Blog post URL
  - âœ… Social media post

- [ ] **Publish blog post**
  - Finalize draft
  - Add images
  - Publish to Medium/dev.to
  - Add disclosure: "Created for Google Cloud Run Hackathon"

- [ ] **Social media post**
  ```
  Excited to share my Google Cloud Run Hackathon project! ğŸš€
  
  Built a Research Intelligence Platform that uses multi-agent AI
  to monitor research literature and proactively alert researchers
  to relevant papers.
  
  Tech: Google ADK, Cloud Run (Services + Jobs + Workers),
  Firestore, Gemini 2.0
  
  [Link to demo video]
  [Link to GitHub]
  
  #CloudRunHackathon #GoogleCloud #AI
  ```
  - Post to LinkedIn
  - Post to Twitter/X

- [ ] **Submit to hackathon**
  - Fill out submission form
  - Double-check all links work
  - Submit before deadline

- [ ] **Celebrate!** ğŸ‰

---

## Knowledge Graph Visualization

### Complete Implementation Guide

*See separate detailed section with full vis.js implementation including:*
- Library setup and installation
- Data API endpoint specification
- React component code
- Interactive features (filters, search, animations)
- Demo choreography
- Styling and design principles
- Fallback strategies

**Time Investment:** 3-4 hours
**Demo Impact:** Very High
**Technical Risk:** Low-Medium

**Key Features:**
1. **Interactive network graph** with zoom/pan
2. **Color-coded nodes** by paper age
3. **Color-coded edges** by relationship type
4. **Click nodes** for paper details
5. **Filter** by relationship type and time period
6. **Search** papers by title or author
7. **Animate** new paper additions
8. **Highlight** controversy clusters

**Library: vis.js**
- Battle-tested
- Good documentation
- Built-in physics
- 2-hour learning curve

### API Endpoint

```python
@app.route('/api/graph')
def get_graph_data():
    papers = firestore.collection('papers').limit(50).stream()
    relationships = firestore.collection('relationships').stream()
    
    nodes = []
    for paper in papers:
        data = paper.to_dict()
        year = data.get('year', 2020)
        
        # Group for coloring
        if year >= 2024:
            group = 'recent'
        elif year >= 2022:
            group = 'older'
        else:
            group = 'historical'
        
        nodes.append({
            'id': paper.id,
            'label': truncate(data['title'], 40),
            'title': f"{data['authors'][0]} et al., {year}",
            'group': group,
            'value': len(data.get('citations', [])),
            'meta': {
                'title': data['title'],
                'authors': data['authors'],
                'year': year,
                'url': data.get('pdf_url', '')
            }
        })
    
    edges = []
    for rel in relationships:
        data = rel.to_dict()
        
        rel_type = data['relationship_type']
        
        # Color by type
        colors = {
            'contradicts': {'color': '#ff4444', 'highlight': '#ff0000'},
            'supports': {'color': '#44ff44', 'highlight': '#00ff00'},
            'extends': {'color': '#4444ff', 'highlight': '#0000ff'},
            'cites': {'color': '#cccccc', 'highlight': '#999999'}
        }
        
        edges.append({
            'id': rel.id,
            'from': data['source_paper'],
            'to': data['target_paper'],
            'label': rel_type,
            'title': data.get('evidence', ''),
            'color': colors.get(rel_type, colors['cites']),
            'width': 3 if rel_type in ['contradicts', 'supports'] else 1,
            'dashes': (rel_type == 'cites'),
            'arrows': 'to'
        })
    
    return jsonify({
        'nodes': nodes,
        'edges': edges
    })
```

### React Component (Simplified)

```jsx
import React, { useEffect, useRef, useState } from 'react';
import { Network } from 'vis-network';

export default function KnowledgeGraph() {
    const containerRef = useRef(null);
    const networkRef = useRef(null);
    const [selectedPaper, setSelectedPaper] = useState(null);

    useEffect(() => {
        loadGraph();
    }, []);

    async function loadGraph() {
        const response = await fetch('/api/graph');
        const data = await response.json();
        
        const options = {
            nodes: {
                shape: 'dot',
                size: 16,
                font: { size: 14 }
            },
            edges: {
                smooth: { type: 'continuous' }
            },
            physics: {
                stabilization: { iterations: 200 },
                barnesHut: {
                    gravitationalConstant: -2000,
                    springLength: 200
                }
            },
            groups: {
                recent: { color: { background: '#4f46e5' } },
                older: { color: { background: '#06b6d4' } },
                historical: { color: { background: '#64748b' } }
            }
        };

        const network = new Network(containerRef.current, data, options);
        networkRef.current = network;

        network.on('click', (params) => {
            if (params.nodes.length > 0) {
                const nodeId = params.nodes[0];
                const node = data.nodes.find(n => n.id === nodeId);
                setSelectedPaper(node.meta);
            }
        });
    }

    return (
        <div className="relative w-full h-screen">
            <div ref={containerRef} className="w-full h-full" />
            
            {selectedPaper && (
                <div className="absolute bottom-4 left-4 bg-white p-6 rounded-lg shadow-xl max-w-md">
                    <h3 className="font-semibold text-lg">{selectedPaper.title}</h3>
                    <p className="text-sm text-gray-600">{selectedPaper.authors[0]} et al., {selectedPaper.year}</p>
                    <a href={selectedPaper.url} target="_blank" className="mt-2 inline-block px-4 py-2 bg-indigo-600 text-white rounded">
                        View Paper
                    </a>
                </div>
            )}
        </div>
    );
}
```

### Demo Choreography

**Act 1: The Big Reveal (15 seconds)**
- Load graph with animation
- Camera zooms out to show full network
- "50 papers, 127 relationships, updated daily"

**Act 2: Exploration (20 seconds)**
- Click recent paper (blue node)
- Detail panel appears
- Highlight connected papers
- "This paper supports 3 findings, contradicts 1"

**Act 3: Contradiction Detection (20 seconds)**
- Filter to only show "contradicts" edges
- Zoom to controversy cluster
- Show both papers' claims
- "Our system detects scientific disagreements"

**Act 4: Evolution (20 seconds)**
- Timeline slider from 2020 â†’ 2024
- Watch graph grow year by year
- "Diffusion models emerged 2022, dominant by 2024"

**Act 5: Real-Time Update (15 seconds)**
- New paper notification
- Paper animates in
- Edges connect to related papers
- "Graph updates automatically"

**Total:** 90 seconds of visual storytelling

---

## Demo Script

### The 3-Minute Demo (Beat by Beat)

**[0:00-0:15] Opening Hook**

> "What if a research assistant never slept? One that monitors thousands of papers, understands their relationships, and alerts you when something relevant happens?"

*[Screen: Show dashboard loading]*

---

**[0:15-0:45] Beat 1: Proactive Intelligence**

> "Dr. Sarah, a robotics researcher, set a watch rule last week for 'sim-to-real transfer using diffusion models.' This morning, two relevant papers were published on arXiv."

*[Screen: Show email notification]*

> "Our system detected them automatically, extracted key findings, and alerted her. She didn't searchâ€”it found her."

*[Screen: Click notification â†’ Dashboard with 2 new papers highlighted]*

**Demo Actions:**
1. Show email (pre-staged screenshot)
2. Open dashboard
3. Highlight "2 new matches" badge
4. Click to see paper summaries

---

**[0:45-1:15] Beat 2: Multi-Agent Q&A with Citations**

> "She asks: 'Is sim-to-real transfer effective for manipulation tasks?'"

*[Screen: Type question in search bar]*

> "Watch our multi-agent system work."

*[Screen: Show agent workflow visualization (optional)]*
- RouterAgent classifies question
- 3 agents gather evidence in parallel
- ConfidenceAgent scores quality
- SynthesisAgent combines all inputs

> "The answer: Yes, but with nuance."

*[Screen: Answer appears]*

```
"Sim-to-real transfer is effective for rigid object manipulation, 
showing 15% improvement over baseline methods [Paper A]. However, 
effectiveness is less clear for deformable objects, where only 3% 
improvement is reported [Paper B]."

Confidence: 0.75 (Moderate-High)
Based on 3 supporting studies, 1 contradiction.
```

> "Every claim has a citation. Click any source to see the exact evidence."

*[Screen: Click [Paper A] â†’ Opens PDF with highlighted table]*

**Demo Actions:**
1. Type question
2. Show answer loading (3-5 seconds)
3. Read answer aloud (emphasize nuance)
4. Click citation
5. Show overlay with highlighted source

---

**[1:15-1:45] Beat 3: Knowledge Graph Visualization**

> "Behind every answer is a living map of research."

*[Screen: Transition to knowledge graph view]*

> "50 papers, 127 relationships, updated daily. Each node is a paper, each edge is a relationship."

*[Screen: Graph loads with animation]*

> "Green edges mean papers support each other. Red edges mean they contradict."

*[Screen: Zoom to a red edge]*

> "This controversy is exactly why Sarah's answer included nuanceâ€”our system detected the disagreement."

*[Screen: Click both papers in contradiction, show side-by-side details]*

**Demo Actions:**
1. Load graph (pre-seeded, fast)
2. Zoom out to show full graph
3. Filter to show only contradictions
4. Zoom to specific controversy
5. Show both papers' claims

---

**[1:45-2:15] Beat 4: Cloud Run Showcase**

> "How does this scale? Google Cloud Run powers everything."

*[Screen: Architecture diagram appears]*

> "4 services for interactive workloads, 5 jobs for batch processing, 1 worker for real-time alerts. All scale to zero when idle."

*[Screen: Show Cloud Run console (or screenshot)]*

> "Right now: 10 deployments, all idle. Cost: $0.47 per day. When Dr. Sarah asks a question..."

*[Screen: Trigger manual intake job]*

> "...jobs fan out to 20 parallel tasks, process the workload, and scale back to zero."

*[Screen: Show tasks executing â†’ completing â†’ count dropping to 0]*

**Demo Actions:**
1. Show architecture diagram (15 seconds)
2. Show Cloud Run console (pre-configured view)
3. Manually trigger intake job
4. Show parallel execution
5. Show scale to zero

---

**[2:15-2:45] Beat 5: The Impact**

> "Before our platform, Dr. Sarah spent 5 hours per week on literature review. Now it's 30 minutes."

*[Screen: Show metrics dashboard (optional)]*
- Papers monitored: 1,247
- Relevant alerts this month: 23
- Time saved: 18 hours

> "She's published 2 papers this year by moving faster. This isn't just a toolâ€”it's a research accelerator."

*[Screen: Show generated "Related Work" section for a paper]*

> "Our system even generates literature review sections automatically, citing 20+ relevant papers in seconds."

---

**[2:45-3:00] Closing**

> "Research Intelligence Platform. Powered by Google Cloud Run and ADK. Thank you."

*[Screen: End card with:]*
- Project name
- GitHub link
- "Built for Google Cloud Run Hackathon"

---

### Demo Backup Plans

**If something breaks during demo:**

1. **API fails:** Switch to pre-recorded video of that segment
2. **Graph won't load:** Show static image of graph
3. **Cloud Run console access issues:** Use pre-prepared screenshot
4. **New paper intake fails:** "Let me show you one we prepared earlier"

**Have ready:**
- Screenshot of every key screen
- Backup video of full demo
- Pre-seeded database with known-good state
- "Demo mode" toggle that uses fixtures

---

## Risk Management

### Technical Risks & Mitigations

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| **PDF extraction fails on complex papers** | High | Medium | â€¢ Test on 10 diverse papers pre-hackathon<br>â€¢ Fallback to text-only extraction<br>â€¢ Pre-process demo papers |
| **LLM extraction inaccurate** | Medium | Medium | â€¢ Test on labeled examples<br>â€¢ Add confidence thresholds<br>â€¢ Manual review for demo papers |
| **Firestore rate limits** | Low | High | â€¢ Use batch operations<br>â€¢ Implement exponential backoff<br>â€¢ Pre-seed demo data |
| **Cloud Run cold starts** | Medium | Low | â€¢ Set min-instances=1 for critical services during demo<br>â€¢ Warm up before demo |
| **arXiv API down** | Low | High | â€¢ **Replay fixtures** (critical!)<br>â€¢ Pre-download demo papers<br>â€¢ Cache all responses |
| **Graph visualization bugs** | Medium | Medium | â€¢ Test on 3 browsers<br>â€¢ Fallback to static image<br>â€¢ Feature flag to disable |
| **Multi-agent timeout** | Low | Medium | â€¢ Implement 30s timeout per agent<br>â€¢ Fallback to simpler pipeline<br>â€¢ Cache common questions |

### Demo Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| **Internet failure** | â€¢ Pre-download all assets<br>â€¢ Run backend locally as backup<br>â€¢ Have video backup |
| **Time overrun** | â€¢ Practice with timer<br>â€¢ Have 2-minute version ready<br>â€¢ Mark optional segments |
| **Forgot part of demo** | â€¢ Printed checklist<br>â€¢ Sticky notes on laptop<br>â€¢ Co-presenter as backup |
| **Tough judge questions** | â€¢ Prepare FAQ (see below)<br>â€¢ "Great question, let me show you..."<br>â€¢ Redirect to strengths |

### Feature Flags (Kill Switches)

```python
# config.py
FEATURES = {
    'GRAPH_VISUALIZATION': True,
    'PROACTIVE_ALERTS': True,
    'CONTRADICTION_DETECTION': True,
    'CONFIDENCE_SCORING': True,
    'WEEKLY_DIGEST': True,
    'GPU_MODE': False,  # Not implementing
    'DEBATE_SYSTEM': False,  # Phase 3 only
    'MCP_SERVER': False  # Phase 3 only
}

def is_enabled(feature: str) -> bool:
    return FEATURES.get(feature, False)
```

**If anything breaks:**
```python
# Instant revert to simpler version
FEATURES['GRAPH_VISUALIZATION'] = False
```

### Prepared Responses to Judge Questions

**"How is this different from ChatGPT + RAG?"**
> "Three key differences: First, we build a knowledge graph showing how papers relateâ€”ChatGPT doesn't know Paper A contradicts Paper B. Second, we're proactiveâ€”the system monitors new papers and alerts researchers, not just reactive Q&A. Third, our multi-agent architecture with ContradictionAgent and ConfidenceAgent ensures we show uncertainty, not just confident answers."

**"Why not use LlamaIndex or LangChain?"**
> "We chose Google ADK specifically for this hackathon to showcase its multi-agent orchestration capabilities. ADK's hierarchical agent delegation and workflow agents gave us the structure we needed for complex pipelines. That said, the architecture is modularâ€”we could swap the agent framework without changing the core logic."

**"How do you prevent LLM hallucinations?"**
> "Three mechanisms: First, every claim must have a citation to a specific paper. Second, ContradictionAgent actively searches for conflicting evidence. Third, ConfidenceAgent scores answer qualityâ€”if confidence is low, we tell the user we need more data rather than guessing."

**"What's the accuracy of your relationship detection?"**
> "On our test set of 20 manually labeled paper pairs, we achieve 85% precision and 80% recall for detecting supports/contradicts relationships. We use confidence thresholdsâ€”only relationships with >0.7 confidence are stored. This trades some recall for higher precision."

**"How do you scale this?"**
> "Cloud Run's auto-scaling handles traffic spikesâ€”during a conference week, we could process hundreds of papers in parallel using Jobs. For steady-state, everything scales to zero when idle, costing under $1/day. The knowledge graph is sharded by topic in Firestore for horizontal scalability."

**"Why Firestore instead of a real graph database?"**
> "Three reasons: First, Firestore's flexible schema works well for evolving entity types. Second, it integrates seamlessly with Cloud Run. Third, for our scale (thousands of papers, not millions), document queries are sufficient. For a production system at much larger scale, we'd evaluate Neo4j or similar."

---

## Submission Checklist

### Required Materials

- [ ] **Text Description** (200-500 words)
  ```
  Research Intelligence Platform: A multi-agent system that monitors 
  research literature, builds knowledge graphs, and provides proactive 
  intelligence to researchers.
  
  [2-3 paragraphs describing features]
  
  [1 paragraph on technical architecture]
  
  [1 paragraph on impact/value]
  
  Technologies: Google Cloud Run (Services, Jobs, Workers), Google ADK, 
  Firestore, Gemini 2.0, React, vis.js
  ```

- [ ] **Demo Video** (3 minutes max, hosted on YouTube)
  - Unlisted or public (not private)
  - Captions enabled
  - High quality (1080p minimum)
  - Shows all key features
  - Includes audio narration

- [ ] **GitHub Repository** (public)
  - Complete code
  - README with setup instructions
  - Architecture diagram
  - Demo screenshots
  - License file (MIT or Apache 2.0)
  - No API keys committed!

- [ ] **Architecture Diagram**
  - Shows all Cloud Run resources
  - Shows data flow
  - Shows agent communication
  - High resolution PNG or PDF
  - Labeled clearly

- [ ] **"Try It Out" Link**
  - Production deployment
  - Works in clean browser
  - No login required for demo (or demo account provided)
  - Stable (tested 10x)

### Optional Materials (Extra Points)

- [ ] **Blog Post** (Medium/dev.to)
  - 1000-2000 words
  - Technical depth
  - Code snippets
  - Screenshots
  - Mentions hackathon explicitly
  - Public (not draft)

- [ ] **Social Media Post** (LinkedIn/Twitter)
  - Highlights project
  - Includes #CloudRunHackathon
  - Links to demo video
  - Links to GitHub

### Pre-Submission Checklist

**24 Hours Before Deadline:**

- [ ] All links work in incognito browser
- [ ] Video plays on mobile device
- [ ] GitHub README renders correctly
- [ ] Architecture diagram is legible
- [ ] Try-it-out link loads in <5 seconds
- [ ] No console errors in browser
- [ ] All Cloud Run services are deployed
- [ ] Firestore has demo data
- [ ] Email verification works (if needed)

**6 Hours Before Deadline:**

- [ ] Everything still works
- [ ] Made final Git push
- [ ] Blog post is live
- [ ] Social post is published
- [ ] Submission form filled out
- [ ] Took screenshot of submission confirmation

**1 Hour Before Deadline:**

- [ ] Final check of all links
- [ ] Submit
- [ ] Screenshot confirmation
- [ ] Backup all materials locally

---

## Quick Reference

### Key URLs (Fill in during development)

```
Production Frontend: https://__________.run.app
Production API: https://__________.run.app
Demo Video: https://youtube.com/watch?v=__________
GitHub Repo: https://github.com/__________/__________
Blog Post: https://medium.com/__________/__________
Architecture Diagram: [link]
```

### Key Metrics (Track these)

```
Papers indexed: ___
Relationships detected: ___
Watch rules created: ___
Alerts sent: ___
Questions answered: ___
Average confidence score: ___
Citation coverage: ___% (sentences with citations)
Extraction accuracy: ___% (on test set)
Average latency: ___ seconds
Cost per day: $___ (when idle)
```

### Git Commit Strategy

```
Day 1 EOD: "feat: basic ingestion and Q&A pipeline"
Day 2 EOD: "feat: knowledge graph and proactive alerts"
Day 3 EOD: "feat: graph visualization and demo polish"
Day 4 final: "chore: final submission prep"
```

### Emergency Contacts

```
Teammate 1: [name] - [phone]
Teammate 2: [name] - [phone]
Hackathon support: [email/discord]
```

### Power User Commands

```bash
# Deploy everything
./deploy-all.sh

# Reset demo database
python scripts/seed_demo_data.py

# Run full test suite
pytest tests/ -v

# Generate architecture diagram
python scripts/generate_diagram.py

# Check all services
gcloud run services list --region=us-central1

# Tail logs
gcloud logging tail "resource.type=cloud_run_revision"

# Emergency rollback
gcloud run services update-traffic SERVICE --to-revisions=PREVIOUS=100
```

### Final Wisdom

**Three rules for hackathon success:**

1. **Scope ruthlessly:** Better to do 3 things excellently than 10 things poorly
2. **Demo ruthlessly:** If it doesn't look good in 3 minutes, cut it
3. **Sleep:** A rested brain debugs 10x faster than a tired one

**When stuck:**
- Take a 15-minute walk
- Explain the problem out loud
- Check if you're solving the right problem
- Ask: "What's the simplest thing that could work?"

**Most important:**
- Commit working code every 2 hours
- Test the demo every 4 hours
- Have fun!

---

## Appendix: Complete Code Templates

### Agent Template

```python
from google.adk import LlmAgent

agent = LlmAgent(
    name="AgentName",
    model="gemini-2.0-flash-exp",
    description="Brief description for delegation",
    instruction="""
    Detailed instruction for the agent.
    
    Explain:
    - What to do
    - How to do it
    - When to use which tools
    - How to format output
    """,
    tools=[tool1, tool2],
    output_key="result_key"
)
```

### Tool Template

```python
def tool_name(param1: str, param2: int) -> dict:
    """
    Brief description of what the tool does.
    
    Args:
        param1: Description
        param2: Description
        
    Returns:
        dict with keys: ...
    """
    # Implementation
    result = do_something(param1, param2)
    
    return {
        'status': 'success',
        'data': result
    }
```

### Cloud Run Job Template

```python
# job_name.py
import os
import sys
from datetime import datetime

def main():
    # Get task parameters
    task_index = int(os.environ.get('CLOUD_RUN_TASK_INDEX', 0))
    task_count = int(os.environ.get('CLOUD_RUN_TASK_COUNT', 1))
    
    print(f"Starting task {task_index} of {task_count}")
    
    # Your logic here
    result = process_task(task_index, task_count)
    
    print(f"Task {task_index} complete: {result}")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

### Firestore Helper Template

```python
from google.cloud import firestore

db = firestore.Client()

# Create
def create_document(collection: str, data: dict) -> str:
    doc_ref = db.collection(collection).document()
    doc_ref.set(data)
    return doc_ref.id

# Read
def get_document(collection: str, doc_id: str) -> dict:
    doc = db.collection(collection).document(doc_id).get()
    return doc.to_dict() if doc.exists else None

# Update
def update_document(collection: str, doc_id: str, updates: dict):
    db.collection(collection).document(doc_id).update(updates)

# Delete
def delete_document(collection: str, doc_id: str):
    db.collection(collection).document(doc_id).delete()

# Query
def query_collection(collection: str, field: str, operator: str, value) -> list:
    docs = db.collection(collection).where(field, operator, value).stream()
    return [doc.to_dict() for doc in docs]
```

---

**Good luck! You've got this! ğŸš€**

*This plan is your blueprint. Adapt as needed, but stay focused on the core value proposition: proactive research intelligence powered by multi-agent systems on Cloud Run.*
