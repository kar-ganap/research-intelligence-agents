#!/usr/bin/env python3
"""
Regenerate All Relationships - With Embedding-Based Filtering

This script implements Options 1, 5, and 6 for addressing graph sparsity:
- Option 1: Embedding-based pre-filtering (reduces comparisons by ~83%)
- Option 5: Selective confidence thresholds by relationship type
- Option 6: Refined prompt that encourages finding meaningful relationships

Process:
1. Load embeddings from cache (generated by generate_embeddings.py)
2. Delete ALL existing relationships from the database
3. For each paper, find top-20 most similar papers using embeddings
4. Detect relationships only among similar papers (not all papers)
5. Apply selective thresholds: contradicts=0.7, extends/supports=0.5

Expected results:
- Reduce API calls from ~1,176 to ~200-300 (83% reduction)
- Increase relationships from ~90 to 150-200 (67-122% increase)
- Improve graph density from 7.7% to 15-17%
"""

import sys
import os
import json
import time
import logging
from pathlib import Path
from typing import List, Dict
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import threading

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.agents.ingestion.relationship_agent import RelationshipAgent
from src.storage.firestore_client import FirestoreClient

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    force=True
)
logger = logging.getLogger(__name__)

# Cache file location
CACHE_DIR = Path(__file__).parent.parent / "cache"
EMBEDDINGS_CACHE_FILE = CACHE_DIR / "paper_embeddings.json"


class RateLimiter:
    """Rate limiter for API calls."""

    def __init__(self, max_calls_per_minute: int = 50):
        self.max_calls = max_calls_per_minute
        self.calls = []
        self.lock = threading.Lock()

    def wait_if_needed(self):
        """Wait if we've hit the rate limit."""
        with self.lock:
            now = time.time()
            # Remove calls older than 1 minute
            self.calls = [t for t in self.calls if now - t < 60]

            if len(self.calls) >= self.max_calls:
                # Need to wait
                oldest = self.calls[0]
                wait_time = 60 - (now - oldest) + 0.1  # Add small buffer
                if wait_time > 0:
                    logger.info(f"Rate limit reached, waiting {wait_time:.1f}s...")
                    time.sleep(wait_time)
                    # Clean up again after waiting
                    now = time.time()
                    self.calls = [t for t in self.calls if now - t < 60]

            # Record this call
            self.calls.append(time.time())


def load_embeddings_cache() -> Dict[str, List[float]]:
    """Load embeddings from cache file."""
    logger.info(f"Loading embeddings from {EMBEDDINGS_CACHE_FILE}...")

    if not EMBEDDINGS_CACHE_FILE.exists():
        raise FileNotFoundError(
            f"Embeddings cache not found: {EMBEDDINGS_CACHE_FILE}\n"
            f"Please run 'python scripts/generate_embeddings.py' first."
        )

    with open(EMBEDDINGS_CACHE_FILE, 'r') as f:
        embeddings = json.load(f)

    logger.info(f"Loaded {len(embeddings)} embeddings from cache")
    return embeddings


def main():
    """Main regeneration process."""
    start_time = time.time()

    logger.info("=" * 80)
    logger.info("REGENERATING RELATIONSHIPS WITH EMBEDDING-BASED FILTERING")
    logger.info("=" * 80)
    logger.info("Improvements:")
    logger.info("  - Option 1: Embedding pre-filtering (top-20 similar papers)")
    logger.info("  - Option 5: Selective thresholds (contradicts=0.7, extends/supports=0.5)")
    logger.info("  - Option 6: Refined prompt (encourages finding relationships)")
    logger.info("=" * 80)
    logger.info("")

    # Initialize clients
    firestore_client = FirestoreClient()
    relationship_agent = RelationshipAgent()
    rate_limiter = RateLimiter(max_calls_per_minute=50)

    # Load embeddings
    embeddings_cache = load_embeddings_cache()

    # Fetch all papers
    logger.info("Fetching papers from Firestore...")
    all_papers = firestore_client.get_all_papers()
    logger.info(f"Found {len(all_papers)} papers in corpus")

    # Sort papers by publication date (oldest first)
    # This ensures we process in chronological order
    all_papers.sort(key=lambda p: p.get('published', ''), reverse=False)

    # Delete existing relationships
    logger.info("Deleting existing relationships...")
    relationships_ref = firestore_client.db.collection('relationships')
    deleted = 0
    for doc in relationships_ref.stream():
        doc.reference.delete()
        deleted += 1

    logger.info(f"Deleted {deleted} existing relationships")
    logger.info("")

    # Regenerate relationships using embedding-based filtering
    logger.info("Starting relationship detection with embedding pre-filtering...")
    logger.info(f"Will process {len(all_papers)} papers")
    logger.info("")

    total_relationships = 0
    total_comparisons = 0
    max_possible_comparisons = len(all_papers) * (len(all_papers) - 1) // 2

    for i, paper in enumerate(all_papers):
        paper_num = i + 1
        logger.info(f"\n[{paper_num}/{len(all_papers)}] Processing: {paper.get('title', 'Unknown')[:60]}...")

        # Use the new filtered method
        relationships = relationship_agent.detect_relationships_batch_filtered(
            new_paper=paper,
            existing_papers=all_papers,
            embeddings_cache=embeddings_cache,
            top_k=20,  # Compare against top-20 most similar papers
            min_similarity=0.6  # Minimum embedding similarity threshold
        )

        # Estimate comparisons made (top-20 similar papers)
        estimated_comparisons = min(20, len(all_papers) - 1)
        total_comparisons += estimated_comparisons

        # Store relationships
        for rel in relationships:
            firestore_client.store_relationship(rel)
            total_relationships += 1

        logger.info(f"  Found {len(relationships)} relationships")
        logger.info(f"  Total relationships so far: {total_relationships}")
        logger.info(f"  Total comparisons: {total_comparisons} "
                   f"(vs {max_possible_comparisons} if no filtering)")

    # Final statistics
    elapsed = time.time() - start_time
    logger.info("")
    logger.info("=" * 80)
    logger.info("REGENERATION COMPLETE")
    logger.info("=" * 80)
    logger.info(f"Total papers processed: {len(all_papers)}")
    logger.info(f"Total relationships created: {total_relationships}")
    logger.info(f"Total comparisons made: {total_comparisons}")
    logger.info(f"Maximum possible comparisons: {max_possible_comparisons}")
    logger.info(f"Reduction in comparisons: {(1 - total_comparisons/max_possible_comparisons)*100:.1f}%")
    logger.info(f"Graph density: {total_relationships/max_possible_comparisons*100:.1f}%")
    logger.info(f"Time elapsed: {elapsed/60:.1f} minutes")
    logger.info("")

    # Relationship type breakdown
    logger.info("Fetching relationship type breakdown...")
    all_rels = list(firestore_client.db.collection('relationships').stream())
    type_counts = {}
    for rel_doc in all_rels:
        rel = rel_doc.to_dict()
        rel_type = rel.get('relationship_type', 'unknown')
        type_counts[rel_type] = type_counts.get(rel_type, 0) + 1

    logger.info("Relationship types:")
    for rel_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  {rel_type}: {count}")

    logger.info("=" * 80)


if __name__ == "__main__":
    main()
